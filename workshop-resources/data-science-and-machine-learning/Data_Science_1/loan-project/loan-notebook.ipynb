{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loan Qualification Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contoso Data helps customers qualify for home loans. They are a global data science company that works in all sectors of housing.  Customers first apply for a home loan after Contoso Data validates the customer eligibility for a loan.\n",
    "\n",
    "In this project, Contoso Data wants to automate the loan eligibility process (real-time) based on customer details provided while filling out an online application form. These details are listed below in the table with the variable names and descriptions. The variables are listed exactly how they are in the data.\n",
    "\n",
    "> A data dictionary is a description of data in business terms, also including information about the data such as data types, details of structure, and security restrictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dictionary - Variable Descriptions\n",
    "\n",
    "|VARIABLE | DESCRIPTION|\n",
    "|---|---|\n",
    "|Loan_ID|Unique Loan ID|\n",
    "|Gender|Male/Female|\n",
    "|Married|Applicant married (Y/N)|\n",
    "|Dependents|Number of dependents|\n",
    "|Education| Applicant Education (Graduate/ Under Graduate)|\n",
    "|Self_Employed|Self employed (Y/N)|\n",
    "|ApplicantIncome|Applicant income|\n",
    "|CoapplicantIncome|Coapplicant income|\n",
    "|LoanAmount|Loan amount in thousands|\n",
    "|Loan_Amount_Term|Term of loan in months|\n",
    "|Credit_History|Credit History meets guidelines|\n",
    "|Property_Area|Urban/ Semi Urban/ Rural|\n",
    "|Loan_Status|Loan approved (Y/N)|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem\n",
    "\n",
    "We will assume to role of data scientist for Contoso Data and approach this problem exactly how we would do it in the real world. A data scientist with expertise in home mortgages would often be called on to create models that classify or determine specific outcomes based on some data they are given. These could be logistic regression models, linear regression, or a custom model that would take a team of experts to develop the inputs, weights, and outcomes. \n",
    "\n",
    "Just as if we actually worked in the Contoso Data Center, we'll create a common yet valuable tool that a loan officer with thousands of applications would use to quickly determine the best candidates for loans. The Loan Officer is our domain expert and would consult a data scientist, such as yourself, to come up with ways to make accurate pre-selections, and also save a great deal of time. \n",
    "\n",
    "Our task will be to create a machine learning model that will predict whether or not a loan will be approved.\n",
    "\n",
    "### Data Science Ethical Practices\n",
    "\n",
    "Both the Certified Analytics Professional (CAP) and the United Nations Statistics Division have released official codes and declarations of ethics to address data science directives. The purpose of these guidelines are to clarify crucially important ethical requirements that set standards, help in deterring deceitful behavior, and keep individuals and organizations accountable for the ways they collect and use data-driven information. \n",
    "\n",
    "It's important that data scientists learn and find training to address ethical issues in data science and within their industries.\n",
    "\n",
    "We can start by taking some responsibility and set some guidelines. Here are some common guidelines that we should use when working with data: 1) collect minimal data and aggregate what is there, in other words, only collect what's needed. 2) Identify and scrub sensitive data, and 3) have a plan in case you make a mistake. \n",
    "\n",
    "### Removing the Gender and Married fields\n",
    "\n",
    "Since we would not discriminate against people based on their gender, we wil remove that field. Also, instead of basing our decision on whether or not someone is married, we'll remove it and use something like income since it would be a more objective variable to base a loan decision on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data Science Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Understanding the domain\n",
    "2. Making a plan\n",
    "3. Exploring the data\n",
    "4. Preparing the data\n",
    "5. Training your model\n",
    "6. Review your results\n",
    "7. Deployment\n",
    "\n",
    "During our workshop we've talked a lot about why understanding the domain you are working in data science is so critical to uncovering the most valuable insights in your data. In other words, you know the business **inside and out**. By knowing your business your experiements will be more efficient and tackle the known issues in your domain. Does this mean that we shouldn't *play* with other datasets? Of course not, in fact, using other data and exploring it can uncover insight and be a great tool for learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset provides you a taste of working on data sets from insurance companies â€“ what challenges are faced there, what strategies are used, which variables influence the outcome, etc. The data has 615 rows and 13 columns. Our plan will be to approach this problem as a **logistical regression** study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin the loan qualification activity we should first load the dataset and the libraries that we will be using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will import the following libraries to aid in our study and activity.\n",
    "\n",
    "* `numpy`\n",
    "* `matplotlib`\n",
    "* `pandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on using `numpy` and `pandas` the pandas website has a tutorial that will cover their usage in [10 minutes.]( https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#Reading the dataset in a dataframe using pandas\n",
    "df = pd.read_csv(\"Data/loan_prediction_training_data.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remember that you can use a question mark after any method or function in the Jupyter notebook. To read more about how to use the `read_csv` function, use the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Explore the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can scroll to the right as well to see all the fields in the dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint: describe() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `describe` function provides count, mean, standard deviation, min, quartiles, and max in its' output. If you need a refresher on some of these values review them in this [article.](https://www.analyticsvidhya.com/blog/2014/07/statistics/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are some inferences you can make about the data looking at the output from the describe function? You should be looking for things like missing values, inconsistencies, and other problems with your data. You might have to reference an domain expert (if you're not one), and find out more information about the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the `describe` function we can see that LoanAmount, Loan_Amount_Term and Credit_History are all missing values. \n",
    "\n",
    "Also, if we recall exploring the output of the `head` function, we saw that Credit_History only has 1 or 0 as values. Looking at the mean value, we see that only 84% of the applicants have credit histories. Do you notice anything else?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-numerical values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's deal with the `Gender` and `Married` columns. We will drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Gender', 'Married'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now that's taken care of--Well Done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the data to a new file\n",
    "df.to_csv('Data/loan_prediction_training_data_no_G_M.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For dealing with non-numerical data (e.g. Property_Area, Education, Self_Employed) we can examine the frequency distribution and try to understand the data more. We can use the `.value_counts()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Self_Employed'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Property_Area'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Education'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a cursory exploration of the data, we should now look at the distribution of the numeric data, specifically ApplicantIncome and LoanAmount. Let's use a histogram to plot a histogram of ApplicantIncome. We know there is a wide range of variation here in the values, so we'll use 50 bins to properly depict the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ApplicantIncome'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use boxplots to better understand the distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(column='ApplicantIncome')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boxplot confirms that we have a lot of outliers in our data. We already know that there is a huge difference between how much money different individuals make, and much of it is due to their education level. Let's break college graduates and non-college graduates out into groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(column='ApplicantIncome', by = 'Education')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do a similar review of the LoanAmount variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LoanAmount'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(column='LoanAmount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(column='LoanAmount', by = 'Education')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extremely interesting and clear results. It's obvious that ApplicantIncome and LoanAmount are going to require some amount of data processing and munging to give us better results. \n",
    "\n",
    "One more plot we should look at before we go is whether or not being self-employed is a factor in a loan approval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(column='LoanAmount', by = 'Self_Employed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are some other boxplots and histograms you would like to examine? Take some time now to do your own investigations on some variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's categorical data? In this case we are referring to non-numerical data such as words as values. What can we do with this sort of data to gain a better understanding? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the chances a person has getting a loan based solely on credit history. We will look at the Loan_Status data which is a boolean value, 0 or 1. We can assume that 0 is for a No, and 1 is for a Yes. We will use a pivot table in pandas to make this visual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First we will create a frequency table on the Credit_History column to see how it is distributed.\n",
    "temp1 = df['Credit_History'].value_counts(ascending=True)\n",
    "\n",
    "# Let's also create a pivot table that shows us the probability of loan approval when you factor in credit history.\n",
    "# Loan_Status is coded 1 for approved and 0 for unapproved, this means the average of all \n",
    "# the values is the probablity of getting a loan.\n",
    "\n",
    "temp2 = df.pivot_table(values='Loan_Status',index=['Credit_History'],aggfunc=lambda x: x.map({'Y':1,'N':0}).mean())\n",
    "\n",
    "print ('Frequency Table for Credit History:') \n",
    "print (temp1)\n",
    "\n",
    "print ('\\nProbability of getting loan based on Credit_History:')\n",
    "print (temp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge: Find the Average of the Loan_Status variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we find the average of the Loan_Status column? Hint: The data is categorical. One way to solve this is to find out how many 'Y' answers there are and then divide that by the total number of values in the column. \n",
    "\n",
    "Hint: Use the `value_counts` function and then just divide the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at plots of this data. Let's create more plots and gain more understanding about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.set_xlabel('Credit_History')\n",
    "ax1.set_ylabel('Count of Applicants')\n",
    "ax1.set_title(\"Applicants by Credit_History\")\n",
    "temp1.plot(kind='bar')\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.set_xlabel('Credit_History')\n",
    "ax2.set_ylabel('Probability of Approval')\n",
    "ax2.set_title(\"Probability of Approval Based on Credit History\")\n",
    "temp2 = temp2.squeeze()\n",
    "temp2.plot(kind = 'bar')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try changing the Credit_History variable to something else like Married, Self_Employed, or Property Area. Do any of the variable closely correlate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked Charts\n",
    "\n",
    "We can also create a stacked chart that combines the plots above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp3 = pd.crosstab(df['Credit_History'], df['Loan_Status'])\n",
    "temp3.plot(kind='bar', stacked=True, color=['red','blue'], grid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp3 = pd.crosstab([df['Credit_History'], df['Education']], df['Loan_Status'],)\n",
    "temp3.plot(kind='bar', stacked=True, color=['red','blue'], )\n",
    "temp3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations!** Did you realize what we just did? We just created two basic **classification algorithms!** We created the first one that is based just on Credit_History, and then we created on based on Credit_History and Education. \n",
    "\n",
    "This is the last step that we'll do in exploring our dataset. From the exploration we know that we need to deal with some messy data. Let's move on to some **Data Munging.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Munging and Preparing our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous part of our process we learned about some of the problems with our data, specifically that we have some missing values. If we are going to \"fix\" these problems we need to be extremely careful and thoughtful about what type of data we will use to fill in the empty values. \n",
    "\n",
    "We noticed that in the ApplicantIncome and LoanAmount columns there was also outlying data on the extreme ends of the ranges. Even though we understand why the values are the way they are, we should do something with the outlying values as well.\n",
    "\n",
    "Let's see what we can do with the `apply` function, first let's examine what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The apply function applies a function across the axis of a dataframe.\n",
    "df.apply?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(lambda x: sum(x.isnull()),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `lambda` function allows us to scan through the dataframe and find all the fields that contain a null value.\n",
    "\n",
    "Note: Even though we don't have a huge number of missing data, we should still do a little more work and estimating if we can fill though values with fill data that makes sense. We should also check for thing that we notice. For example, Loan_Amount_Term has 14 null values. If a loan is made with a loan period of 0, is it null? Let's explore a bit more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling null values in LoanAmount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figuring what to fill null values with can be tricky, and you should always refer to a domain expert if possible. But in this case, we can probably use the average loan amout to fill the null values. We will do that first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LoanAmount'].fillna(df['LoanAmount'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check and see, we shouldn't see any null values in the LoanAmount column\n",
    "df.apply(lambda x: sum(x.isnull()),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can figure out what the fill value for loan amount is using value_counts\n",
    "df['LoanAmount'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling values in the Self_Employed Column\n",
    "\n",
    "From our exploration step, we know that there are quite a few null values in the self employed column. Let's figure out a good way to fill the null values based on what we know about the data we do have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(df.boxplot(column='LoanAmount', by = 'Education'))\n",
    "print(df.boxplot(column='LoanAmount', by = 'Self_Employed'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these boxplots to spot trends in our data, and we see some variation between the median loan amount so that can be used to impute the data. Let's verify that Self_Employed and Education should not have any null or missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Self_Employed'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now we can calculate the percentage of 'No' responses\n",
    "print('Percentage of \"No\" values in the data is:', 500/582*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since about 86% of the responses in this column are a No, it is safe for us to impute the missing values as \"no\" values. We can do that with this code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Self_Employed'].fillna('No',inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create a pivot table, that will provide us the median values for all the groups of unique values of Self_Employed and Education features. Next, we define a function, which returns the values of these cells and apply it to fill the missing values of loan amount:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = df.pivot_table(values='LoanAmount', index='Self_Employed', columns='Education', aggfunc=np.median)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to return value of this pivot_table\n",
    "def fage(x):\n",
    " return table.loc[x['Self_Employed'],x['Education']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values\n",
    "df['LoanAmount'].fillna(df.apply(fage, axis=1), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a log transformation to nullify the effect of extreme values\n",
    "\n",
    "Let's look at LoanAmount first. We already know that people apply for loans in all ranges, including high value loans for specific properties. Instead of treating them as outliers, we can use a log transformation to reduce the effect they have on representing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pull the histgram again\n",
    "df['LoanAmount_log'] = np.log(df['LoanAmount'])\n",
    "df['LoanAmount_log'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this distribution looks better. The effect that the higher limit values has been considerably reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing to note when considering ApplicantIncome. Did you notice that there was also a CoapplicantIncome? It might be a good idea to combine these columns into a TotalIncome column and do a log transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TotalIncome'] = df['ApplicantIncome'] + df['CoapplicantIncome']\n",
    "df['TotalIncome_log'] = np.log(df['TotalIncome'])\n",
    "df['TotalIncome_log'].hist(bins=20) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution again is better than before. You can decide whether or not you will continue the munging exercise with Dependents or the other variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a predictive model in Python\n",
    "\n",
    "So far we've spent a lot of time prepping our data getting it ready for our model. We'll be using a new library (for us) to code our model. Scikit-learn (sklearn) is the most commonly used data science library in Python for this purpose.\n",
    "\n",
    "Skicit-Learn requires that all inputs be numeric, but first let's quickly fill in all the null values within our data. \n",
    "\n",
    "For the sake of time, we've prepared all the code for you here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Fill of all the null values in the data\n",
    "df['Dependents'].fillna(df['Dependents'].mode()[0], inplace=True)\n",
    "df['Loan_Amount_Term'].fillna(df['Loan_Amount_Term'].mode()[0], inplace=True)\n",
    "df['Credit_History'].fillna(df['Credit_History'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here were are using LabelEncoder to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "var_mod = ['Dependents','Education','Self_Employed','Property_Area','Loan_Status']\n",
    "le = LabelEncoder()\n",
    "for i in var_mod:\n",
    "    df[i] = le.fit_transform(df[i])\n",
    "#fancy huh?    \n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's check! All taken care of.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now we can finish the model\n",
    "# Import models from scikit learn module:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "#Generic function for making a model and accessing performance:\n",
    "def loan_model(model, data, predictors, outcome):\n",
    "  #Fit the model:\n",
    "  model.fit(data[predictors],data[outcome])\n",
    "  \n",
    "  #Make predictions on training set:\n",
    "  predictions = model.predict(data[predictors])\n",
    "  \n",
    "  #Print accuracy\n",
    "  accuracy = metrics.accuracy_score(predictions,data[outcome])\n",
    "  print (\"Accuracy : %s\" % \"{0:.3%}\".format(accuracy))\n",
    "\n",
    "  #Fit the model again so that it can be refered outside the function:\n",
    "  model.fit(data[predictors],data[outcome]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a logistical regression model\n",
    "\n",
    "Remember that logistical regression is a model that returns a binary answer. In this case it will return whether or not a loan is approved based on the parameters provided. \n",
    "\n",
    "We want to create a model that generalizes well. If we take all the data and use that to train our model, we run into the risk of 'overfitting' the model. \n",
    "\n",
    "Let's first start by making some simple hypothesis about how someone's chances of getting a loan will be higher.\n",
    "\n",
    "1. We know already having a credit history is huge.\n",
    "2. Higher incomes, combining coapplicant and applicant incomes will help.\n",
    "3. We also saw that applicants with higher education get loans.\n",
    "4. We also know properties in high growth locations will make better loans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's first work with Credit_History - We start by assigning Loan_Status as the outcome variable\n",
    "outcome_var = 'Loan_Status'\n",
    "\n",
    "# Select the Model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Use credit history\n",
    "predictor_var = ['Credit_History']\n",
    "\n",
    "# call the model\n",
    "loan_model(model, df, predictor_var, outcome_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS:  Decision Tree and Random Forest\n",
    "\n",
    "A decision tree is another predictive model. We can easily import the model from the sklearn library. In addition we can also do the same thing for the Random Forest model, which is a classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "predictor_var = ['Credit_History','Dependents','Self_Employed','Education']\n",
    "loan_model(model, df,predictor_var,outcome_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you do any better? Play with the code and try to get a higher accuracy value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One possible solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's try using the RandomForestClassifier model. Random Forest Classifier\n",
    "# A random forest is a meta estimator that fits a number of decision tree classifiers \n",
    "# on various sub-samples of the dataset and uses averaging to improve the predictive \n",
    "# accuracy and control over-fitting. \n",
    "model = RandomForestClassifier(n_estimators=10) # n_estimators == number of trees in the forest\n",
    "predictor_var = ['Dependents', 'Education',\n",
    "       'Self_Employed', 'Credit_History', 'Property_Area',\n",
    "        'LoanAmount_log']\n",
    "loan_model(model, df,predictor_var,outcome_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "1. Using a more sophisticated model does not guarantee better results.\n",
    "2. Avoid using complex modeling techniques as a black box without understanding the underlying concepts. Doing so would increase the tendency of overfitting thus making your models less interpretable\n",
    "3. Feature Engineering is the key to success. Everyone can use the prebuilt models but the real art and creativity lies in enhancing your features to better suit the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}